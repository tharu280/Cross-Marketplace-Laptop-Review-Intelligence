{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d457db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AgentInbox/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f067bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index path: /Users/dilshantharushika/Desktop/laptop agent/backend/laptops.index\n",
      "Metadata path: /Users/dilshantharushika/Desktop/laptop agent/backend/laptops_metadata.json\n",
      "Using Embedding Model: all-MiniLM-L6-v2\n",
      "Using LLM: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "BACKEND_DIR = '/Users/dilshantharushika/Desktop/laptop agent/backend' \n",
    "INDEX_PATH = os.path.join(BACKEND_DIR, 'laptops.index')\n",
    "METADATA_PATH = os.path.join(BACKEND_DIR, 'laptops_metadata.json')\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' \n",
    "GEMINI_MODEL_NAME = 'gemini-2.5-flash' \n",
    "print(f\"FAISS index path: {os.path.abspath(INDEX_PATH)}\")\n",
    "print(f\"Metadata path: {os.path.abspath(METADATA_PATH)}\")\n",
    "print(f\"Using Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Using LLM: {GEMINI_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea39bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAG artifacts\n",
      "RAG artifacts loaded successfully!\n",
      "Index contains 291 vectors.\n",
      "Metadata contains 291 entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading RAG artifacts\")\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    faiss_index = faiss.read_index(INDEX_PATH)\n",
    "    with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        metadata_store = json.load(f)\n",
    "    print(\"RAG artifacts loaded successfully!\")\n",
    "    print(f\"Index contains {faiss_index.ntotal} vectors.\")\n",
    "    print(f\"Metadata contains {len(metadata_store)} entries.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading RAG artifacts: {e}\")\n",
    "    print(f\"Make sure '{INDEX_PATH}' and '{METADATA_PATH}' exist.\")\n",
    "    embedding_model = None\n",
    "    faiss_index = None\n",
    "    metadata_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa83a90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Google Generative AI client...\n",
      "Google client configured successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring Google Generative AI client...\")\n",
    "\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"Error: GOOGLE_API_KEY not found.\")\n",
    "    print(\"Please create a .env file in the same directory as this notebook and add:\")\n",
    "    print(\"GOOGLE_API_KEY=your-google-api-key\")\n",
    "    llm_model = None \n",
    "else:\n",
    "    try:\n",
    "        genai.configure(api_key=google_api_key)\n",
    "        llm_model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "        print(\"Google client configured successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring Google client (is your API key correct?): {e}\")\n",
    "        llm_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda1149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_system(query, k=4):\n",
    "    \"\"\"\n",
    "    Performs the full Retrieve, Augment, Generate (RAG) pipeline using Google Gemini.\n",
    "    \"\"\"\n",
    "   \n",
    "    if not all([embedding_model, faiss_index, metadata_store, llm_model]):\n",
    "        print(\"Error: One or more components (embedding model, index, metadata, LLM) failed to load.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Query: {query} \")\n",
    "    \n",
    "\n",
    "    start_retrieve = time.time()\n",
    "    query_vector = embedding_model.encode([query]).astype('float32')\n",
    "    distances, indices = faiss_index.search(query_vector, k)\n",
    "    retrieved_chunks = [metadata_store[i] for i in indices[0]]\n",
    "    end_retrieve = time.time()\n",
    "    print(f\"Retrieval took {end_retrieve - start_retrieve:.2f} seconds.\")\n",
    "\n",
    "    \n",
    "    context_string = \"\"\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        context_string += f\"Context {i+1}:\\n\"\n",
    "        context_string += f\"  Source: {chunk['sku']}\\n\"\n",
    "        context_string += f\"  Content: {chunk['text']}\\n\"\n",
    "        context_string += f\"  Citations: {chunk['citations']}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Q&A assistant for laptop specifications.\n",
    "    Your answers must be accurate and directly based on the provided context only.\n",
    "    Do not use any outside knowledge.\n",
    "    When you use information, you MUST cite the 'Citations' number provided with the context (e.g.,).\n",
    "\n",
    "    Here is the context retrieved from the database:\n",
    "    --- START CONTEXT ---\n",
    "    {context_string}\n",
    "    --- END CONTEXT ---\n",
    "\n",
    "    Based *only* on the context provided, please answer the following question:\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(\"Sending to Google Gemini API...\")\n",
    "    start_generate = time.time()\n",
    "    try:\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.0,\n",
    "            max_output_tokens=512\n",
    "        )\n",
    "        response = llm_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        answer = response.text.strip()\n",
    "        end_generate = time.time()\n",
    "        \n",
    "        print(f\"Generation took {end_generate - start_generate:.2f} seconds.\")\n",
    "        print(\"--- LLM Answer ---\")\n",
    "        print(answer)\n",
    "        \n",
    "        print(\"\\n--- Context Provided to LLM ---\")\n",
    "        print(context_string)\n",
    "\n",
    "    except Exception as e:\n",
    "        end_generate = time.time()\n",
    "        print(f\"Generation failed after {end_generate - start_generate:.2f} seconds.\")\n",
    "        print(f\"Error during Google Gemini API call: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4546336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query: What is the maximum memory supported by the Thinkpad E14 Gen 5 Intel? \n",
      "Retrieval took 0.34 seconds.\n",
      "Sending to Google Gemini API...\n",
      "Generation took 4.83 seconds.\n",
      "--- LLM Answer ---\n",
      "The ThinkPad E14 Gen 5 (Intel) supports a maximum memory of up to 48GB (16GB soldered + 32GB SO-DIMM) DDR4-3200 [65, 71]. It is noted that 48GB is for technical readiness testing, and available memory to sell may vary [65, 71].\n",
      "\n",
      "--- Context Provided to LLM ---\n",
      "Context 1:\n",
      "  Source: HP ProBook 450 G10 â€” Datasheet\n",
      "  Content: Maximum memory: 32 GB DDR4-3200 MHz RAM; (Transfer rates up to 3200 MT/s.); 7 Both slots are accessible/upgradeable by IT or self-maintainers only. Supports dual channel memory. Memory slots: 2 SODIMM.\n",
      "  Citations: []\n",
      "\n",
      "Context 2:\n",
      "  Source: Lenovo ThinkPad E14 Gen 5 (AMD)\n",
      "  Content: Max Memory: Up to 40GB (8GB soldered + 32GB SO-DIMM) DDR4-3200. Memory Slots: One memory soldered to systemboard, one DDR4 SO-DIMM slot, dual-channel capable. Memory Type: DDR4-3200.\n",
      "  Citations: []\n",
      "\n",
      "Context 3:\n",
      "  Source: ThinkPad E14 Gen 5 (Intel)\n",
      "  Content: Maximum Memory: Up to 48GB (16GB soldered + 32GB SO-DIMM) DDR4-3200 (48GB is to test technical readiness; available memory to sell will vary)\n",
      "  Citations: [65, 71]\n",
      "\n",
      "Context 4:\n",
      "  Source: HP ProBook 440 14 inch G11 Notebook PC\n",
      "  Content: Maximum: 32 GB DDR5-5600 MT/s RAM\n",
      "  Citations: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_rag_system(\"What is the maximum memory supported by the Thinkpad E14 Gen 5 Intel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374d9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgentInbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
